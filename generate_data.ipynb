{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMPIRE_PATH = '/home/apluska/.vampire/bin/vampire_z3_rel_static_casc2023_6749'\n",
    "TPTP_PATH = '/home/apluska/TPTP-v8.2.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select problems which are at most 100_000 bytes in size, have functions with at most arity 8, and at most 16 functions of each arity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apluska/miniconda3/envs/foreduce/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Selected 0/12 Problems, parsing AGT/AGT007+1.p:   0%|          | 12/25963 [00:01<38:06, 11.35it/s]"
     ]
    }
   ],
   "source": [
    "from foreduce.tptp.parser import read_file\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "total, success = 0, 0\n",
    "num_variables, num_functions = 0, []\n",
    "for dir, file in (pbar := tqdm([(dir, file) for dir in sorted(os.listdir(TPTP_PATH + 'Problems')) for file in sorted(os.listdir(TPTP_PATH + 'Problems/' + dir))])):\n",
    "    current = file\n",
    "    pbar.set_description(f'Selected {success}/{total} Problems, parsing {dir}/{file}')\n",
    "    if not file.endswith('.p'):\n",
    "        continue\n",
    "    try:\n",
    "        total += 1\n",
    "        problem = read_file(TPTP_PATH + 'Problems/' + dir + '/' + file, include_path=TPTP_PATH, max_size=100_000)\n",
    "        _variables = max(len(clause.variables()) for clause in problem.clauses)\n",
    "        num_variables = max(num_variables, _variables)\n",
    "        _symbols = []\n",
    "        for s in problem.function_symbols() | problem.predicate_symbols():\n",
    "            if s.arity > 8:\n",
    "                break\n",
    "            if len(_symbols) <= s.arity:\n",
    "                _symbols += [0 for _ in range(s.arity + 1 - len(_symbols))]\n",
    "            _symbols[s.arity] += 1\n",
    "        else:\n",
    "            if any(count > 16 for count in _symbols):\n",
    "                continue\n",
    "            num_functions = [max(a, b) for a, b in zip_longest(num_functions, _symbols, fillvalue=0)]\n",
    "            success += 1\n",
    "            os.makedirs('./problems/' + dir, exist_ok=True)\n",
    "            with open('./problems/' + dir + '/' + file, 'w') as f:\n",
    "                f.write(problem.to_tptp())\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f'Maximum number of variables: {num_variables}')\n",
    "print(f'Maximum number of functions of each arity: {num_functions}')\n",
    "\n",
    "with open('problems/stats.txt', 'w') as f:\n",
    "    f.write(f'Total problems: {total}\\n')\n",
    "    f.write(f'Successfully parsed problems: {success}\\n')\n",
    "    f.write(f'Maximum number of variables: {num_variables}\\n')\n",
    "    f.write(f'Maximum number of functions of each arity: {num_functions}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4906/25963 have been selected. The maximum number of functions of each respective arity is [16, 16, 16, 16, 9, 4, 3, 4, 5].\n",
    "\n",
    "Next, we generate proofs for these problems using vampire with a timeout of 1. We limit ourselves to proofs which are at most 1_000_000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Succesfully proved 0/0 Problems, proving ALG/ALG002-1.p:   0%|          | 0/4906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Succesfully proved 1807/4902 Problems, proving TOP/TOP022+1.p: 100%|██████████| 4906/4906 [53:41<00:00,  1.52it/s]        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4903, 1807)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "with open('problems.txt', 'r') as f:\n",
    "    problems = f.read().split('\\n')\n",
    "\n",
    "total, success = 0, 0\n",
    "for dir, file in (pbar := tqdm([(dir, file) for dir in sorted(os.listdir('./problems')) for file in sorted(os.listdir('./problems/' + dir))])):\n",
    "    pbar.set_description(f'Succesfully proved {success}/{total} Problems, proving {dir}/{file}')\n",
    "    args = [VAMPIRE_PATH, './problems/' + dir + '/' + file,  '--show_new', 'on', '-t', '1', '--avatar', 'off', '--proof', 'off']\n",
    "    try:\n",
    "        result = subprocess.run(args, capture_output=True, text=True, timeout=5)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        continue\n",
    "    if result.returncode == 0:\n",
    "        if 'Refutation found.' in result.stdout:\n",
    "            success += 1    \n",
    "            os.makedirs('./proofs/' + dir, exist_ok=True)\n",
    "            with open('./proofs/' + dir + '/' + file, 'w') as f:\n",
    "                f.write(result.stdout)\n",
    "    total += 1\n",
    "\n",
    "total, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manage to prove about half the selected problems.\n",
    "\n",
    "Let's bring our proofs into tensor form. We fix a maximum number of 1024 steps per proof and 128 tokens per clause. We will be generating 32 data points per problem, i.e. 4MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing proof ALG/ALG002-1.p:   0%|          | 0/1807 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting proof of SET/SET578+3.p to 54/64 datapoints:  68%|██████▊   | 1224/1807 [11:27:51<5:27:37, 33.72s/it]    \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(datapoints_per_proof):\n\u001b[1;32m     20\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConverting proof of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatapoints_per_proof\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m datapoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m         dataset\u001b[38;5;241m.\u001b[39madd_proof(problem, tree, goal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./proofs.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximal encountered variables in a clause: \u001b[39m\u001b[38;5;132;01m{config.num_variables}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/foreduce/foreduce/data/data.py:21\u001b[0m, in \u001b[0;36mVampireProofs.add_proof\u001b[0;34m(self, problem, tree, mapping, goal)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_proof\u001b[39m(\u001b[38;5;28mself\u001b[39m, problem, tree, mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, goal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     20\u001b[0m     x, y, target, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(problem, tree, mapping, goal)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget, target\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_functions = [16, 16, 16, 16, 9, 4, 3, 4, 5]\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from foreduce.data.data import VampireProofs\n",
    "from foreduce.transformer.tokenizer import TokenConfig\n",
    "from foreduce.vampire.parser import read_file\n",
    "\n",
    "config = TokenConfig(num_functions=num_functions)\n",
    "dataset = VampireProofs(config=config, max_steps=1024, max_tokens=128)\n",
    "\n",
    "datapoints_per_proof = 64\n",
    "\n",
    "for dir, file in (pbar := tqdm([(dir, file) for dir in sorted(os.listdir('./proofs')) for file in sorted(os.listdir('./proofs/' + dir))])):\n",
    "    pbar.set_description(f'Parsing proof {dir}/{file}')\n",
    "    problem, tree = read_file('./proofs/' + dir + '/' + file)\n",
    "    for i in range(datapoints_per_proof):\n",
    "        pbar.set_description(f'Converting proof of {dir}/{file} to {i+1}/{datapoints_per_proof} datapoints')\n",
    "        dataset.add_proof(problem, tree, goal='random')\n",
    "\n",
    "torch.save(dataset, './proofs.pt')\n",
    "\n",
    "print(\"Maximal encountered variables in a clause: {config.num_variables}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78389"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from foreduce.data.data import VampireProofs\n",
    "\n",
    "dataset = torch.load('./proofs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenConfig(RESERVED_TOKENS=9, reserved_token_mapping={'<PAD>': 0, '<START>': 1, '<END>': 2, '|': 3, '~': 4, '$true': 5, '$false': 6, 'eq': 7}, num_functions=[16, 16, 16, 16, 9, 4, 3, 4, 5], num_variables=10, embed_dim=128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of proofs.pt: 56.97 GB\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of proofs.pt: {os.path.getsize(\"./proofs.pt\") / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding(dataset[:\u001b[38;5;241m10\u001b[39m][\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/foreduce/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/foreduce/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/foreduce/foreduce/transformer/embedding.py:59\u001b[0m, in \u001b[0;36mFormulaEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mExpects formulas as a BxPxL tensor, where B is the batch size, i.e. number of problems,\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mP is the size of each problem andand L is the length of each formula.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m B, P, L \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(x)\n\u001b[1;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional(x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m)))\u001b[38;5;241m.\u001b[39mview(B, P, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/foreduce/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/foreduce/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/foreduce/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/foreduce/lib/python3.12/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "embedding(dataset[:10][2].unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.7955e-02,  1.6495e-02, -4.3709e-02,  ...,  5.4811e-01,\n",
       "           2.7829e+00,  1.0368e-01],\n",
       "         [ 2.7564e-02,  2.7573e-02, -4.4436e-02,  ...,  5.6219e-01,\n",
       "           2.7765e+00,  1.0271e-01],\n",
       "         [ 2.5892e-02,  7.8649e-02, -1.6624e-02,  ...,  5.1412e-01,\n",
       "           2.6276e+00,  7.5360e-02],\n",
       "         ...,\n",
       "         [ 4.5653e-02,  1.5125e-02, -8.3644e-03,  ...,  6.0137e-01,\n",
       "           2.6398e+00,  3.7518e-02],\n",
       "         [ 2.0547e-02,  3.6807e-02, -4.1588e-02,  ...,  5.8795e-01,\n",
       "           2.4859e+00,  3.0267e-02],\n",
       "         [ 1.6676e-02,  2.1576e-02,  1.3228e-02,  ...,  5.8944e-01,\n",
       "           2.5555e+00,  4.3205e-02]],\n",
       "\n",
       "        [[-1.6349e-02,  1.1977e-02, -3.1936e-02,  ...,  5.4543e-01,\n",
       "           2.7698e+00,  8.2195e-02],\n",
       "         [-1.8308e-02, -4.4173e-02,  1.2420e-02,  ...,  5.3383e-01,\n",
       "           2.7574e+00,  7.8418e-02],\n",
       "         [-1.8347e-02,  4.8654e-02, -3.6250e-02,  ...,  5.6356e-01,\n",
       "           2.6051e+00,  6.3861e-02],\n",
       "         ...,\n",
       "         [-4.1270e-02, -1.7999e-02, -1.3162e-02,  ...,  6.0618e-01,\n",
       "           2.6272e+00,  8.0052e-02],\n",
       "         [ 1.0184e-02,  1.7148e-03, -3.9059e-02,  ...,  5.4725e-01,\n",
       "           2.4469e+00,  7.2700e-02],\n",
       "         [-2.7333e-02, -2.8599e-02,  2.1770e-02,  ...,  5.6992e-01,\n",
       "           2.5464e+00,  8.1719e-02]],\n",
       "\n",
       "        [[-1.1997e-02,  1.1034e-03, -4.1692e-02,  ...,  5.4524e-01,\n",
       "           2.7546e+00,  8.5461e-02],\n",
       "         [-1.2081e-02, -3.0601e-02, -1.8939e-02,  ...,  5.2023e-01,\n",
       "           2.7493e+00,  7.1025e-02],\n",
       "         [ 3.0406e-02,  3.9069e-02, -7.6424e-02,  ...,  5.3861e-01,\n",
       "           2.5861e+00,  7.2004e-02],\n",
       "         ...,\n",
       "         [ 4.2375e-02, -1.0060e-02, -4.9917e-02,  ...,  5.1661e-01,\n",
       "           2.6638e+00,  4.1271e-02],\n",
       "         [ 2.9883e-02,  4.4306e-02, -9.0314e-02,  ...,  4.6534e-01,\n",
       "           2.4890e+00,  4.3718e-02],\n",
       "         [ 1.7523e-03, -1.5705e-02, -2.4435e-03,  ...,  4.9920e-01,\n",
       "           2.5762e+00,  5.3775e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 7.5837e-03,  9.5150e-03, -5.5310e-02,  ...,  5.4965e-01,\n",
       "           2.7679e+00,  6.6900e-02],\n",
       "         [-3.8625e-03, -4.1768e-03, -4.2500e-02,  ...,  5.3843e-01,\n",
       "           2.7639e+00,  3.5469e-02],\n",
       "         [ 1.4914e-02,  4.3644e-02, -2.9953e-02,  ...,  5.3165e-01,\n",
       "           2.5978e+00,  5.1554e-02],\n",
       "         ...,\n",
       "         [-1.2224e-02,  2.1519e-02,  5.4637e-03,  ...,  5.4372e-01,\n",
       "           2.6215e+00,  2.0941e-02],\n",
       "         [-1.2611e-02,  1.5405e-02, -3.8746e-02,  ...,  5.4755e-01,\n",
       "           2.4587e+00,  1.7537e-02],\n",
       "         [-1.3232e-02,  1.5023e-02,  1.5508e-03,  ...,  5.5034e-01,\n",
       "           2.5416e+00,  3.7807e-02]],\n",
       "\n",
       "        [[ 6.1940e-05,  1.9884e-02, -4.5609e-02,  ...,  5.5669e-01,\n",
       "           2.7795e+00,  7.7042e-02],\n",
       "         [ 3.7830e-03, -2.8076e-04, -2.3822e-02,  ...,  5.4780e-01,\n",
       "           2.7830e+00,  7.2320e-02],\n",
       "         [-1.8347e-02,  4.8654e-02, -3.6250e-02,  ...,  5.6356e-01,\n",
       "           2.6051e+00,  6.3861e-02],\n",
       "         ...,\n",
       "         [-1.0315e-02,  1.3609e-02, -3.8038e-02,  ...,  5.7858e-01,\n",
       "           2.6456e+00,  1.0753e-01],\n",
       "         [-3.8270e-03,  2.5777e-02, -4.8201e-02,  ...,  5.5061e-01,\n",
       "           2.4775e+00,  1.0048e-01],\n",
       "         [-1.1678e-02,  1.2646e-02,  8.3529e-03,  ...,  5.5546e-01,\n",
       "           2.5732e+00,  1.0400e-01]],\n",
       "\n",
       "        [[ 9.3952e-03,  2.1813e-02, -4.1141e-02,  ...,  5.4500e-01,\n",
       "           2.7651e+00,  1.0068e-01],\n",
       "         [-4.0895e-03, -4.8599e-03,  3.6590e-03,  ...,  5.3546e-01,\n",
       "           2.7552e+00,  1.0003e-01],\n",
       "         [ 8.4647e-03,  6.1326e-02, -4.0197e-02,  ...,  5.4764e-01,\n",
       "           2.6310e+00,  7.0670e-02],\n",
       "         ...,\n",
       "         [-7.0371e-05, -1.6577e-02, -1.6137e-02,  ...,  5.9095e-01,\n",
       "           2.6719e+00,  4.5576e-02],\n",
       "         [ 1.6569e-02, -4.4586e-05, -2.6120e-02,  ...,  5.3337e-01,\n",
       "           2.4766e+00,  1.6242e-02],\n",
       "         [-4.2838e-03, -2.0081e-02,  3.0361e-02,  ...,  5.5582e-01,\n",
       "           2.5620e+00,  4.8051e-02]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "pool = nn.AdaptiveAvgPool2d((1, 128))\n",
    "pool(embedding(dataset[:10][0])).squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>achievable(west(m(s(s(X12())))c(s(s(s(s(X12()))))))boatonwest()east(m(X20())c(X23())))<END>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted = {v: k for k, v in mapping.items()}\n",
    "\n",
    "input = [inverted[i] if i in inverted else f\"X{i}\" for i in goal.tolist()]\n",
    "result = \"\"\n",
    "for i in range(input):\n",
    "    result += input[i]\n",
    "    if input[i+1] not in [\"(\", \")\"]:\n",
    "        result += \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to extract data from the proofs. Again, we only go for proofs with less than 1_000_000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed 29/43, curently parsing RNG/RNG011-5.p: 100%|██████████| 1/1 [00:21<00:00, 21.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.vampire.parser import parse_string\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "success, total = 0, 0\n",
    "attempts = []\n",
    "for dir in (pbar := tqdm(os.listdir('./proofs/'))):\n",
    "    for file in os.listdir('./proofs/' + dir):\n",
    "        pbar.set_description(f'Parsed {success}/{total}, curently parsing {dir}/{file}')\n",
    "        total += 1\n",
    "        if os.path.getsize('./proofs/' + dir + '/' + file) > 1_000_000:\n",
    "            continue\n",
    "        with open('./proofs/' + dir + '/' + file, 'r') as f:\n",
    "            problem = f.read()\n",
    "        success += 1\n",
    "        attempts.append(parse_string(problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "          [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "          [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "          [-0.3012,  1.3818,  0.8491,  1.1310]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "         [[-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "          [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "          [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "          [-0.3012,  1.3818,  0.8491,  1.1310]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "import torch\n",
    "\n",
    "rotary = RotaryPositionalEmbeddings(4, base=50)\n",
    "x = torch.ones(2, 2, 4, 4)\n",
    "rotary(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apluska/miniconda3/envs/foreduce/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from foreduce.transformer.model import Model\n",
    "from foreduce.data.data import VampireProofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset = torch.load('./proofs_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model({\n",
    "        \"clause_length\": 128,\n",
    "        \"clause_num_heads\": 4,\n",
    "        \"clause_embed_layers\": 4,\n",
    "        \"clause_embed_dim\": 64,\n",
    "        \"problem_length\": 1024,\n",
    "        \"problem_num_heads\": 8,\n",
    "        \"problem_embed_layers\": 8,\n",
    "        \"problem_embed_dim\": 512,\n",
    "    },\n",
    "    dataset.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(dataset[:10][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.5000, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(model(dataset[:10][0]), dataset[:10][1].to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 1,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo-reduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
