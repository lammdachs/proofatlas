{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMPIRE_PATH = '/home/apluska/.vampire/bin/vampire_z3_rel_static_casc2023_6749'\n",
    "TPTP_PATH = '/home/apluska/TPTP-v8.2.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select problems which are at most 100_000 bytes in size, have at most 16 variables per clause, functions with at most arity 8, and at most 16 functions of each arity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected 0/4 Problems, currently parsing RNG/RNG025-9.p:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected 8468/25473 Problems, currently parsing SEV/SEV514^1.p: 100%|██████████| 55/55 [06:30<00:00,  7.11s/it]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of variables: 16\n",
      "Maximum number of functions of each arity: [16, 16, 16, 16, 9, 4, 4, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.tptp.parser import read_file\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "total, success = 0, 0\n",
    "variables, functions = 0, []\n",
    "problems = []\n",
    "for dir in (pbar := tqdm(os.listdir(TPTP_PATH + 'Problems'))):\n",
    "    for file in os.listdir(TPTP_PATH + 'Problems/' + dir):\n",
    "        current = file\n",
    "        pbar.set_description(f'Selected {success}/{total} Problems, currently parsing {dir}/{file}')\n",
    "        if not file.endswith('.p'):\n",
    "            continue\n",
    "        try:\n",
    "            total += 1\n",
    "            problem = read_file(TPTP_PATH + 'Problems/' + dir + '/' + file, include_path=TPTP_PATH, max_size=100_000)\n",
    "            success += 1\n",
    "            _variables = max(len(clause.variables()) for clause in problem.clauses)\n",
    "            if _variables > 16:\n",
    "                continue\n",
    "            variables = max(variables, _variables)\n",
    "            _functions = []\n",
    "            for f in problem.function_symbols() | problem.predicate_symbols():\n",
    "                if f.arity > 8:\n",
    "                    continue\n",
    "                if len(_functions) <= f.arity:\n",
    "                    _functions += [0 for _ in range(f.arity + 1 - len(_functions))]\n",
    "                _functions[f.arity] += 1\n",
    "            if any(a > 16 for a in _functions):\n",
    "                continue\n",
    "            functions = [max(a, b) for a, b in zip_longest(functions, _functions, fillvalue=0)]\n",
    "            problems.append(dir + '/' + file)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "with open('problems.txt', 'w') as f:\n",
    "    f.write('\\n'.join(problems))\n",
    "\n",
    "print(f'Maximum number of variables: {variables}')\n",
    "print(f'Maximum number of functions of each arity: {functions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8468/25473 have been selected. The maximum number of functions of each respective arity is [16, 16, 16, 16, 9, 4, 4, 5, 5].\n",
    "\n",
    "Next, we generate proofs for these problems using vampire with a timeout of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Succesfully proved 4500/8949 Problems, currently proving SEV/SEV436-1.p: 100%|██████████| 8954/8954 [1:31:18<00:00,  1.63it/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8950, 4501)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "with open('problems.txt', 'r') as f:\n",
    "    problems = f.read().split('\\n')\n",
    "\n",
    "total, success = 0, 0\n",
    "for problem in (pbar := tqdm(problems)):\n",
    "    pbar.set_description(f'Succesfully proved {success}/{total} Problems, currently proving {problem}')\n",
    "    args = [VAMPIRE_PATH, TPTP_PATH + 'Problems/' + problem,  '--show_new', 'on', '--include', TPTP_PATH, '-t', '1', '--avatar', 'off', '--proof', 'off']\n",
    "    try:\n",
    "        result = subprocess.run(args, capture_output=True, text=True, timeout=5)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        continue\n",
    "    if result.returncode == 0:\n",
    "        success += 1    \n",
    "        os.makedirs(os.path.dirname('./proofs/' + problem), exist_ok=True)\n",
    "        with open('./proofs/' + problem, 'w') as f:\n",
    "            f.write(result.stdout)\n",
    "    total += 1\n",
    "pbar.set_description(f'Succesfully proved {success}/{total} Problems')\n",
    "\n",
    "total, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manage to prove a bit over half the selected problems.\n",
    "\n",
    "Let's define two functions which let us transform the proofs into tensors. We fix a maximum number of 1024 steps per proof and 128 tokens per clause. We will be generating 64 data points per problem, i.e. 8MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from foreduce.transformer.tokenizer import TokenConfig\n",
    "\n",
    "MAX_STEPS = 1024; MAX_TOKENS = 128\n",
    "\n",
    "config = TokenConfig(num_variables=16, num_functions=[16, 16, 16, 16, 9, 4, 4, 5, 5])\n",
    "\n",
    "def get_tensor(problem, tree):\n",
    "    assert len(problem.clauses) == len(tree)\n",
    "    assert len(tree) <= MAX_STEPS\n",
    "    \n",
    "    tokens = problem.tokenize(config)\n",
    "    x = torch.zeros(MAX_STEPS, MAX_TOKENS)\n",
    "    y = torch.zeros(MAX_STEPS)\n",
    "    for i, clause in enumerate(tokens[:MAX_STEPS]):\n",
    "        for j, token in enumerate(clause[:MAX_TOKENS]):\n",
    "            x[i, j] = token\n",
    "    queue = [len(tree) - 1]\n",
    "    while queue:\n",
    "        i = queue.pop()\n",
    "        y[i] = 1\n",
    "        queue += tree[i]\n",
    "    return x, y\n",
    "\n",
    "from foreduce.vampire.parser import read_file\n",
    "\n",
    "problem, tree = read_file('./proofs/PUZ/PUZ008-1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tensor(problem, tree)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to extract data from the proofs. Again, we only go for proofs with less than 1_000_000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed 29/43, curently parsing RNG/RNG011-5.p: 100%|██████████| 1/1 [00:21<00:00, 21.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.vampire.parser import parse_string\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "success, total = 0, 0\n",
    "attempts = []\n",
    "for dir in (pbar := tqdm(os.listdir('./proofs/'))):\n",
    "    for file in os.listdir('./proofs/' + dir):\n",
    "        pbar.set_description(f'Parsed {success}/{total}, curently parsing {dir}/{file}')\n",
    "        total += 1\n",
    "        if os.path.getsize('./proofs/' + dir + '/' + file) > 1_000_000:\n",
    "            continue\n",
    "        with open('./proofs/' + dir + '/' + file, 'r') as f:\n",
    "            problem = f.read()\n",
    "        success += 1\n",
    "        attempts.append(parse_string(problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "         [-1.3254,  0.4932,  0.6812,  1.2394],\n",
       "         [-1.1311, -0.8489,  0.4997,  1.3230],\n",
       "         [ 0.1032, -1.4104,  0.3082,  1.3802],\n",
       "         [ 1.2426, -0.6753,  0.1106,  1.4099],\n",
       "         [ 1.2396,  0.6808, -0.0892,  1.4114]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "         [-1.3254,  0.4932,  0.6812,  1.2394],\n",
       "         [-1.1311, -0.8489,  0.4997,  1.3230],\n",
       "         [ 0.1032, -1.4104,  0.3082,  1.3802],\n",
       "         [ 1.2426, -0.6753,  0.1106,  1.4099],\n",
       "         [ 1.2396,  0.6808, -0.0892,  1.4114]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "import torch\n",
    "\n",
    "rotary = RotaryPositionalEmbeddings(4, base=50)\n",
    "x = torch.ones(2, 7, 4)\n",
    "rotary(x.view(2, 7, 1, -1)).view(2, 7, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo-reduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
