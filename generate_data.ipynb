{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMPIRE_PATH = '/home/apluska/.vampire/bin/vampire_z3_rel_static_casc2023_6749'\n",
    "TPTP_PATH = '/home/apluska/TPTP-v8.2.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select problems which are at most 100_000 bytes in size, have at most 16 variables per clause, functions with at most arity 8, and at most 16 functions of each arity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected 0/4 Problems, currently parsing RNG/RNG025-9.p:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected 8468/25473 Problems, currently parsing SEV/SEV514^1.p: 100%|██████████| 55/55 [06:30<00:00,  7.11s/it]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of variables: 16\n",
      "Maximum number of functions of each arity: [16, 16, 16, 16, 9, 4, 4, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.tptp.parser import read_file\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "total, success = 0, 0\n",
    "variables, functions = 0, []\n",
    "problems = []\n",
    "for dir in (pbar := tqdm(os.listdir(TPTP_PATH + 'Problems'))):\n",
    "    for file in os.listdir(TPTP_PATH + 'Problems/' + dir):\n",
    "        current = file\n",
    "        pbar.set_description(f'Selected {success}/{total} Problems, currently parsing {dir}/{file}')\n",
    "        if not file.endswith('.p'):\n",
    "            continue\n",
    "        try:\n",
    "            total += 1\n",
    "            problem = read_file(TPTP_PATH + 'Problems/' + dir + '/' + file, include_path=TPTP_PATH, max_size=100_000)\n",
    "            success += 1\n",
    "            _variables = max(len(clause.variables()) for clause in problem.clauses)\n",
    "            if _variables > 16:\n",
    "                continue\n",
    "            variables = max(variables, _variables)\n",
    "            _functions = []\n",
    "            for f in problem.function_symbols() | problem.predicate_symbols():\n",
    "                if f.arity > 8:\n",
    "                    continue\n",
    "                if len(_functions) <= f.arity:\n",
    "                    _functions += [0 for _ in range(f.arity + 1 - len(_functions))]\n",
    "                _functions[f.arity] += 1\n",
    "            if any(a > 16 for a in _functions):\n",
    "                continue\n",
    "            functions = [max(a, b) for a, b in zip_longest(functions, _functions, fillvalue=0)]\n",
    "            problems.append(dir + '/' + file)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "with open('problems.txt', 'w') as f:\n",
    "    f.write('\\n'.join(problems))\n",
    "\n",
    "print(f'Maximum number of variables: {variables}')\n",
    "print(f'Maximum number of functions of each arity: {functions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8468/25473 have been selected. The maximum number of functions of each respective arity is [16, 16, 16, 16, 9, 4, 4, 5, 5].\n",
    "\n",
    "Next, we generate proofs for these problems using vampire with a timeout of 1. We limit ourselves to proofs which are at most 1_000_000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Succesfully proved 4500/8949 Problems, currently proving SEV/SEV436-1.p: 100%|██████████| 8954/8954 [1:31:18<00:00,  1.63it/s]      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8950, 4501)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "with open('problems.txt', 'r') as f:\n",
    "    problems = f.read().split('\\n')\n",
    "\n",
    "total, success = 0, 0\n",
    "for problem in (pbar := tqdm(problems)):\n",
    "    pbar.set_description(f'Succesfully proved {success}/{total} Problems, currently proving {problem}')\n",
    "    args = [VAMPIRE_PATH, TPTP_PATH + 'Problems/' + problem,  '--show_new', 'on', '--include', TPTP_PATH, '-t', '1', '--avatar', 'off', '--proof', 'off']\n",
    "    try:\n",
    "        result = subprocess.run(args, capture_output=True, text=True, timeout=5)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        continue\n",
    "    if result.returncode == 0:\n",
    "        if len(result.stdout) < 1_000_000:\n",
    "            success += 1    \n",
    "            os.makedirs(os.path.dirname('./proofs/' + problem), exist_ok=True)\n",
    "            with open('./proofs/' + problem, 'w') as f:\n",
    "                f.write(result.stdout)\n",
    "    total += 1\n",
    "pbar.set_description(f'Succesfully proved {success}/{total} Problems')\n",
    "\n",
    "total, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manage to prove a bit over half the selected problems.\n",
    "\n",
    "Let's bring our proofs into tensor form. We fix a maximum number of 1024 steps per proof and 128 tokens per clause. We will be generating 64 data points per problem, i.e. 8MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m         problem, tree \u001b[38;5;241m=\u001b[39m read_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./proofs/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m             dataset\u001b[38;5;241m.\u001b[39madd_proof(problem, tree)\n\u001b[1;32m     15\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproofs.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/foreduce/foreduce/data/data.py:20\u001b[0m, in \u001b[0;36mVampireProofs.add_proof\u001b[0;34m(self, problem, tree, mapping, goal)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_proof\u001b[39m(\u001b[38;5;28mself\u001b[39m, problem, tree, mapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, goal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m     x, y, target, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(problem, tree, mapping, goal)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[0;32m~/foreduce/foreduce/transformer/tokenizer.py:60\u001b[0m, in \u001b[0;36mProofTokenizer.__call__\u001b[0;34m(self, problem, tree, mapping, goal)\u001b[0m\n\u001b[1;32m     58\u001b[0m         x[i, j] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m     59\u001b[0m queue \u001b[38;5;241m=\u001b[39m [goal]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m queue:\n\u001b[1;32m     61\u001b[0m     i \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from foreduce.data.data import VampireProofs\n",
    "from foreduce.transformer.tokenizer import TokenConfig\n",
    "from foreduce.vampire.parser import read_file\n",
    "import os\n",
    "\n",
    "config = TokenConfig(num_variables=16, num_functions=[16, 16, 16, 16, 9, 4, 4, 5, 5])\n",
    "dataset = VampireProofs(config=config, max_steps=1024, max_tokens=128)\n",
    "\n",
    "for dir in os.listdir('./proofs')[:1]:\n",
    "    for file in os.listdir('./proofs/' + dir)[:2]:\n",
    "        problem, tree = read_file('./proofs/' + dir + '/' + file)\n",
    "        for _ in range(64):\n",
    "            dataset.add_proof(problem, tree)\n",
    "\n",
    "dataset.save('proofs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem, tree = read_file('./proofs/GEO/GEO002-1.p')\n",
    "dataset.add_proof(problem, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>achievable(west(m(s(s(X12())))c(s(s(s(s(X12()))))))boatonwest()east(m(X20())c(X23())))<END>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted = {v: k for k, v in mapping.items()}\n",
    "\n",
    "input = [inverted[i] if i in inverted else f\"X{i}\" for i in goal.tolist()]\n",
    "result = \"\"\n",
    "for i in range(input):\n",
    "    result += input[i]\n",
    "    if input[i+1] not in [\"(\", \")\"]:\n",
    "        result += \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to extract data from the proofs. Again, we only go for proofs with less than 1_000_000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed 29/43, curently parsing RNG/RNG011-5.p: 100%|██████████| 1/1 [00:21<00:00, 21.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.vampire.parser import parse_string\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "success, total = 0, 0\n",
    "attempts = []\n",
    "for dir in (pbar := tqdm(os.listdir('./proofs/'))):\n",
    "    for file in os.listdir('./proofs/' + dir):\n",
    "        pbar.set_description(f'Parsed {success}/{total}, curently parsing {dir}/{file}')\n",
    "        total += 1\n",
    "        if os.path.getsize('./proofs/' + dir + '/' + file) > 1_000_000:\n",
    "            continue\n",
    "        with open('./proofs/' + dir + '/' + file, 'r') as f:\n",
    "            problem = f.read()\n",
    "        success += 1\n",
    "        attempts.append(parse_string(problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "         [-1.3254,  0.4932,  0.6812,  1.2394],\n",
       "         [-1.1311, -0.8489,  0.4997,  1.3230],\n",
       "         [ 0.1032, -1.4104,  0.3082,  1.3802],\n",
       "         [ 1.2426, -0.6753,  0.1106,  1.4099],\n",
       "         [ 1.2396,  0.6808, -0.0892,  1.4114]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [-0.3012,  1.3818,  0.8491,  1.1310],\n",
       "         [-1.3254,  0.4932,  0.6812,  1.2394],\n",
       "         [-1.1311, -0.8489,  0.4997,  1.3230],\n",
       "         [ 0.1032, -1.4104,  0.3082,  1.3802],\n",
       "         [ 1.2426, -0.6753,  0.1106,  1.4099],\n",
       "         [ 1.2396,  0.6808, -0.0892,  1.4114]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "import torch\n",
    "\n",
    "rotary = RotaryPositionalEmbeddings(4, base=50)\n",
    "x = torch.ones(2, 7, 4)\n",
    "rotary(x.view(2, 7, 1, -1)).view(2, 7, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo-reduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
