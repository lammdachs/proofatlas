{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAMPIRE_PATH = '/home/apluska/.vampire/bin/vampire_z3_rel_static_casc2023_6749'\n",
    "TPTP_PATH = '/home/apluska/TPTP-v8.2.0/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select problems which are at most 100_000 bytes in size, have functions with at most arity 8, and at most 16 functions of each arity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected 0/0 Problems, parsing AGT/AGT001+1.p:   0%|          | 0/25963 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Selected 4906/25473 Problems, parsing TOP/TOP053-1.p: 100%|██████████| 25963/25963 [07:42<00:00, 56.14it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of variables: 96\n",
      "Maximum number of functions of each arity: [16, 16, 16, 16, 9, 4, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.tptp.parser import read_file\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "total, success = 0, 0\n",
    "num_variables, num_functions = 0, []\n",
    "for dir, file in (pbar := tqdm([(dir, file) for dir in sorted(os.listdir(TPTP_PATH + 'Problems')) for file in sorted(os.listdir(TPTP_PATH + 'Problems/' + dir))])):\n",
    "    current = file\n",
    "    pbar.set_description(f'Selected {success}/{total} Problems, parsing {dir}/{file}')\n",
    "    if not file.endswith('.p'):\n",
    "        continue\n",
    "    try:\n",
    "        total += 1\n",
    "        problem = read_file(TPTP_PATH + 'Problems/' + dir + '/' + file, include_path=TPTP_PATH, max_size=100_000)\n",
    "        _variables = max(len(clause.variables()) for clause in problem.clauses)\n",
    "        num_variables = max(num_variables, _variables)\n",
    "        _symbols = []\n",
    "        for s in problem.function_symbols() | problem.predicate_symbols():\n",
    "            if s.arity > 8:\n",
    "                break\n",
    "            if len(_symbols) <= s.arity:\n",
    "                _symbols += [0 for _ in range(s.arity + 1 - len(_symbols))]\n",
    "            _symbols[s.arity] += 1\n",
    "        else:\n",
    "            if any(count > 16 for count in _symbols):\n",
    "                continue\n",
    "            num_functions = [max(a, b) for a, b in zip_longest(num_functions, _symbols, fillvalue=0)]\n",
    "            success += 1\n",
    "            os.makedirs('./problems/' + dir, exist_ok=True)\n",
    "            with open('./problems/' + dir + '/' + file, 'w') as f:\n",
    "                f.write(problem.to_tptp())\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f'Maximum number of variables: {num_variables}')\n",
    "print(f'Maximum number of functions of each arity: {num_functions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4906/25963 have been selected. The maximum number of functions of each respective arity is [16, 16, 16, 16, 9, 4, 3, 4, 5].\n",
    "\n",
    "Next, we generate proofs for these problems using vampire with a timeout of 1. We limit ourselves to proofs which are at most 1_000_000 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Succesfully proved 0/0 Problems, proving ALG/ALG002-1.p:   0%|          | 0/4906 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Succesfully proved 1807/4902 Problems, proving TOP/TOP022+1.p: 100%|██████████| 4906/4906 [53:41<00:00,  1.52it/s]        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4903, 1807)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "with open('problems.txt', 'r') as f:\n",
    "    problems = f.read().split('\\n')\n",
    "\n",
    "total, success = 0, 0\n",
    "for dir, file in (pbar := tqdm([(dir, file) for dir in sorted(os.listdir('./problems')) for file in sorted(os.listdir('./problems/' + dir))])):\n",
    "    pbar.set_description(f'Succesfully proved {success}/{total} Problems, proving {dir}/{file}')\n",
    "    args = [VAMPIRE_PATH, './problems/' + dir + '/' + file,  '--show_new', 'on', '-t', '1', '--avatar', 'off', '--proof', 'off']\n",
    "    try:\n",
    "        result = subprocess.run(args, capture_output=True, text=True, timeout=5)\n",
    "    except subprocess.TimeoutExpired:\n",
    "        continue\n",
    "    if result.returncode == 0:\n",
    "        if 'Refutation found.' in result.stdout:\n",
    "            success += 1    \n",
    "            os.makedirs('./proofs/' + dir, exist_ok=True)\n",
    "            with open('./proofs/' + dir + '/' + file, 'w') as f:\n",
    "                f.write(result.stdout)\n",
    "    total += 1\n",
    "\n",
    "total, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manage to prove about half the selected problems.\n",
    "\n",
    "Let's bring our proofs into tensor form. We fix a maximum number of 1024 steps per proof and 128 tokens per clause. We will be generating 64 data points per problem, i.e. 8MB of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting proof of TOP/TOP021+1.p to 64/64 datapoints: 100%|██████████| 1807/1807 [22:18:28<00:00, 44.44s/it]          \n"
     ]
    }
   ],
   "source": [
    "#num_functions = [16, 16, 16, 16, 9, 4, 3, 4, 5]\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from foreduce.data.data import VampireProofs\n",
    "from foreduce.transformer.tokenizer import TokenConfig\n",
    "from foreduce.vampire.parser import read_file\n",
    "\n",
    "config = TokenConfig(num_functions=num_functions)\n",
    "dataset = VampireProofs(config=config, max_steps=1024, max_tokens=128)\n",
    "\n",
    "datapoints_per_proof = 64\n",
    "\n",
    "for dir, file in (pbar := tqdm([(dir, file) for dir in sorted(os.listdir('./proofs')) for file in sorted(os.listdir('./proofs/' + dir))])):\n",
    "    pbar.set_description(f'Parsing proof {dir}/{file}')\n",
    "    problem, tree = read_file('./proofs/' + dir + '/' + file)\n",
    "    for i in range(datapoints_per_proof):\n",
    "        pbar.set_description(f'Converting proof of {dir}/{file} to {i+1}/{datapoints_per_proof} datapoints')\n",
    "        dataset.add_proof(problem, tree)\n",
    "\n",
    "torch.save(dataset, './proofs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115648"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = torch.load('./proofs.pt')\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of proofs.pt: 56.97 GB\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of proofs.pt: {os.path.getsize(\"./proofs.pt\") / 1024**3:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1.,  61., 100.,  ...,   0.,   0.,   0.],\n",
       "         [  1.,   4.,  61.,  ...,   0.,   0.,   0.],\n",
       "         [  1.,  61.,  32.,  ...,   0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  1.,  61.,  32.,  ...,   0.,   0.,   0.],\n",
       "         [  1.,  61.,  32.,  ...,   0.,   0.,   0.],\n",
       "         [  1.,  61.,  32.,  ...,   0.,   0.,   0.]]),\n",
       " tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([  1.,   4.,  61., 100., 100.,  23.,   3.,  61.,  32.,  32.,  32.,  32.,\n",
       "          32.,  32., 100.,  32.,  32.,  32.,  32.,  32.,  32., 100.,  23.,   2.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START>achievable(west(m(s(s(X12())))c(s(s(s(s(X12()))))))boatonwest()east(m(X20())c(X23())))<END>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted = {v: k for k, v in mapping.items()}\n",
    "\n",
    "input = [inverted[i] if i in inverted else f\"X{i}\" for i in goal.tolist()]\n",
    "result = \"\"\n",
    "for i in range(input):\n",
    "    result += input[i]\n",
    "    if input[i+1] not in [\"(\", \")\"]:\n",
    "        result += \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to extract data from the proofs. Again, we only go for proofs with less than 1_000_000 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed 29/43, curently parsing RNG/RNG011-5.p: 100%|██████████| 1/1 [00:21<00:00, 21.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from foreduce.vampire.parser import parse_string\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "success, total = 0, 0\n",
    "attempts = []\n",
    "for dir in (pbar := tqdm(os.listdir('./proofs/'))):\n",
    "    for file in os.listdir('./proofs/' + dir):\n",
    "        pbar.set_description(f'Parsed {success}/{total}, curently parsing {dir}/{file}')\n",
    "        total += 1\n",
    "        if os.path.getsize('./proofs/' + dir + '/' + file) > 1_000_000:\n",
    "            continue\n",
    "        with open('./proofs/' + dir + '/' + file, 'r') as f:\n",
    "            problem = f.read()\n",
    "        success += 1\n",
    "        attempts.append(parse_string(problem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  0.0000, -0.8415,  0.5403],\n",
       "         [ 0.0000,  0.0000, -0.9093, -0.4161],\n",
       "         [ 0.0000,  0.0000, -0.1411, -0.9900],\n",
       "         [ 0.0000,  0.0000,  0.7568, -0.6536],\n",
       "         [ 0.0000,  0.0000,  0.9589,  0.2837],\n",
       "         [ 0.0000,  0.0000,  0.2794,  0.9602]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
       "         [ 0.0000,  0.0000, -0.8415,  0.5403],\n",
       "         [ 0.0000,  0.0000, -0.9093, -0.4161],\n",
       "         [ 0.0000,  0.0000, -0.1411, -0.9900],\n",
       "         [ 0.0000,  0.0000,  0.7568, -0.6536],\n",
       "         [ 0.0000,  0.0000,  0.9589,  0.2837],\n",
       "         [ 0.0000,  0.0000,  0.2794,  0.9602]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "import torch\n",
    "\n",
    "rotary = RotaryPositionalEmbeddings(4, base=50)\n",
    "x = torch.cat([torch.zeros(2, 7, 3), torch.ones(2, 7, 1)], dim=-1)\n",
    "rotary(x[:, :, [2, 3, 0, 1]].view(2, 7, 1, -1)).view(2, 7, -1)[:, :, [2, 3, 0, 1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fo-reduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
