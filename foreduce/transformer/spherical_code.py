# J. H. Conway, R. H. Hardin, and N. J. A. Sloane, Packing lines, planes, etc.: packings in Grassmannian spaces, Experiment. Math. 5 (1996), 139â€“159

import torch

class SphericalCode(torch.nn.Module):
    def __init__(self, dim=8, requires_grad=False):
        super().__init__()
        self.dim = dim
        self.embedding = torch.nn.Embedding(33, dim, padding_idx=0)
        self.embedding.weight.requires_grad = requires_grad
        self.embedding.weight[:, :8] = torch.tensor([
            [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],
            [ 0.,  0.,  0.,  0.,  0., -1.,  1.,  1.],
            [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  1.],
            [ 0.,  0.,  0.,  0.,  0.,  1.,  1., -1.],
            [ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.],
            [ 0.,  0.,  0., -1.,  1.,  0.,  0.,  1.],
            [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  1.],
            [ 0.,  0.,  0.,  1.,  1.,  0.,  0., -1.],
            [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.],
            [ 0.,  0., -1.,  0.,  1.,  0.,  1.,  0.],
            [ 0.,  0.,  1.,  0., -1.,  0.,  1.,  0.],
            [ 0.,  0.,  1.,  0.,  1.,  0., -1.,  0.],
            [ 0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.],
            [ 0., -1.,  0.,  0.,  1.,  1.,  0.,  0.],
            [ 0.,  1.,  0.,  0., -1.,  1.,  0.,  0.],
            [ 0.,  1.,  0.,  0.,  1., -1.,  0.,  0.],
            [ 0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.],
            [ 0., -1.,  1.,  1.,  0.,  0.,  0.,  0.],
            [ 0.,  1., -1.,  1.,  0.,  0.,  0.,  0.],
            [ 0.,  1.,  1., -1.,  0.,  0.,  0.,  0.],
            [ 0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],
            [-1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.],
            [ 1.,  0.,  0., -1.,  0.,  0.,  1.,  0.],
            [ 1.,  0.,  0.,  1.,  0.,  0., -1.,  0.],
            [ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.],
            [-1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],
            [ 1.,  0., -1.,  0.,  0.,  1.,  0.,  0.],
            [ 1.,  0.,  1.,  0.,  0., -1.,  0.,  0.],
            [ 1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],
            [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.],
            [ 1., -1.,  0.,  0.,  0.,  0.,  0.,  1.],
            [ 1.,  1.,  0.,  0.,  0.,  0.,  0., -1.],
            [ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  1.]
        ], dtype=torch.float)

    def forward(self, x):
        return self.embedding(x)

