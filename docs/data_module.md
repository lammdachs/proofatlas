# Data Module Documentation

The `data` module manages datasets of theorem proving problems and proofs.

## Overview

The data module provides:
- **Problemset**: Collections of theorem proving problems
- **Proofset**: Collections of proofs generated by running selectors
- **DatasetConfig**: Configuration management for datasets
- **DatasetManager**: Central registry for multiple datasets

## Module Structure

### config.py

Defines configuration classes for datasets.

```python
@dataclass
class DatasetSplit:
    name: str                    # Split name (train/val/test)
    files: List[str]            # Explicit file list
    patterns: List[str]         # Glob patterns for files
    ratio: Optional[float]      # Split ratio

@dataclass 
class DatasetConfig:
    name: str                   # Dataset name
    file_format: str           # File format (e.g., 'tptp')
    data_format: str           # Data encoding format (e.g., 'graph')
    base_path: Path            # Base directory for files
    splits: List[DatasetSplit] # Train/val/test splits
```

### problemset.py

Dataset class for loading theorem proving problems.

```python
class Problemset(Dataset):
    def __init__(self, config: DatasetConfig, split_name: str = 'train')
    
    def __len__(self) -> int
    def __getitem__(self, idx) -> Tuple[encoded_data, metadata]
    
    def get_problem(self, idx: int) -> Problem
    def get_proof_state(self, idx: int) -> ProofState
    def get_clauses(self, idx: int) -> List[Clause]
```

Features:
- Lazy loading with caching
- Support for file patterns and explicit file lists
- Integration with file format handlers
- PyTorch Dataset compatibility

### proofset.py

Dataset of proofs for training clause selectors.

```python
class Proofset(Dataset):
    def create(self, problemset: Problemset, loop: Loop, 
               selector: Selector, steps: int)
    
    def __len__(self) -> int
    def __getitem__(self, idx) -> Tuple[states, selections, metadata]
    
    def save(self, path: Path)
    def load(self, path: Path)
```

Features:
- Generate proofs by running selectors on problems
- Store proof trajectories for training
- Support for different data encodings
- Serialization for offline training

### manager.py

Central manager for dataset configurations.

```python
class DatasetManager:
    def add_dataset(self, config: DatasetConfig)
    def get_dataset(self, name: str, split: str) -> Problemset
    def list_datasets(self) -> List[str]
    def save_config(self, config: DatasetConfig)
```

## Usage Examples

### Creating a Dataset Configuration

```python
from proofatlas.data import DatasetConfig, DatasetSplit

config = DatasetConfig(
    name="my_dataset",
    file_format="tptp",
    data_format="graph",
    base_path=Path("/data/problems"),
    splits=[
        DatasetSplit(name="train", patterns=["train/*.p"], ratio=0.8),
        DatasetSplit(name="val", patterns=["val/*.p"], ratio=0.1),
        DatasetSplit(name="test", patterns=["test/*.p"], ratio=0.1)
    ]
)
```

### Loading Problems

```python
from proofatlas.data import Problemset

# Load from config
dataset = Problemset(config, split_name='train')

# Access problems
for i in range(len(dataset)):
    encoded, metadata = dataset[i]
    problem = dataset.get_problem(i)
    state = dataset.get_proof_state(i)
```

### Generating Proofs

```python
from proofatlas.data import Proofset
from proofatlas.loops import get_loop
from proofatlas.selectors import get_selector

# Create components
problemset = Problemset(config, 'train')
loop = get_loop('basic')
selector = get_selector('fifo')

# Generate proofs
proofset = Proofset("training_proofs")
proofset.create(problemset, loop, selector, steps=100)

# Save for later use
proofset.save(Path("proofs.pt"))
```

### Using DatasetManager

```python
from proofatlas.data import DatasetManager

# Create manager
manager = DatasetManager(config_dir=Path("configs/datasets"))

# List available datasets
print(manager.list_datasets())

# Get a dataset
train_data = manager.get_dataset("tptp_sample", split="train")

# Create new dataset from files
config = manager.create_dataset_from_files(
    name="custom",
    file_format="tptp",
    data_format="graph",
    base_path=Path("/data"),
    train_files=["p1.p", "p2.p"],
    val_files=["v1.p"]
)
```

## Integration

The data module integrates with:
- **FileFormats**: For parsing problem files
- **DataFormats**: For encoding states for ML models
- **Loops**: For proof generation
- **Selectors**: For clause selection during proof generation
- **Core**: Uses Problem, Clause, ProofState types

## Configuration Files

Dataset configurations can be saved as YAML:

```yaml
name: tptp_sample
file_format: tptp
data_format: graph
base_path: /data/tptp
splits:
  - name: train
    patterns:
      - "Problems/*/train/*.p"
    ratio: 0.8
  - name: val
    files:
      - "Problems/SET/SET001+1.p"
      - "Problems/SET/SET002+1.p"
    ratio: 0.1
metadata:
  version: "1.0"
  description: "Sample TPTP dataset"
```