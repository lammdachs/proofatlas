# Standard training configuration

# Optimization
optimizer:
  type: "AdamW"
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning rate scheduling
scheduler:
  type: "cosine"
  warmup_steps: 1000
  min_lr: 1e-6
  
# Training settings
epochs: 100
gradient_clip_val: 1.0
gradient_accumulation_steps: 1
val_check_interval: 1.0
limit_train_batches: 1.0
limit_val_batches: 1.0

# Early stopping
early_stopping:
  enabled: true
  monitor: "val_loss"
  patience: 10
  mode: "min"
  
# Model checkpointing
checkpointing:
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3
  save_last: true
  
# Mixed precision
precision: 16

# Distributed training
strategy: "ddp"  # Options: dp, ddp, ddp_spawn, deepspeed
num_nodes: 1
devices: "auto"