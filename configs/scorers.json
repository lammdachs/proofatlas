{
  "architectures": {
    "mlp": {
      "type": "mlp",
      "hidden_dim": 64,
      "num_layers": 2
    },

    "attention": {
      "type": "attention",
      "num_heads": 4
    },

    "transformer": {
      "type": "transformer",
      "hidden_dim": 64,
      "num_layers": 2,
      "num_heads": 4
    }
  }
}
